{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicios de razonar\n",
    "Para estos ejercicios no hace falta escribir ni una línea de código. Simplemente hay que responder a las preguntas.\n",
    "\n",
    "### Pregunta 1\n",
    "**Si entrenas 5 modelos diferentes y obtienes en los 5 un 95% de acierto, ¿sería posible combinar los 5 modelos para mejorar ese porcentaje?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entiendo que si es posible combinar esos 5 modelos con ensemble y que el modelo mejorará un poco."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pregunta 2\n",
    "**¿Cuál es la diferencia entre un hard y un soft voting classifier?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hard voting coge la predicción con el mayor número de votos (moda), mientras que soft voting implica combinar las probabilidades de cada predicción en cada modelo y elegir la predicción con la probabilidad total más alta."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pregunta 3\n",
    "**Para mejorar los tiempos de entrenamiento, se suelen paralelizar los procesos de ejecución en el ordenador (un proceso por core) para que entrene más rápido. ¿Sería posible paralelizar el entrenamiento de los algoritmos de bagging vistos en clase? ¿Y los de boosting?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con bagging si es posible, ya que se entrenan independientemente, dan cada uno sus resultados y no interactuan entre ellos.   \n",
    "Con Boosting no es posible, porque necesitas los resultados del anterior para entrenar el siguiente (aunque hay algunas excepciones).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pregunta 4\n",
    "**¿Qué podríamos modificar en nuestro algoritmo de AdaBoost para mejorarlo en el caso de que se produzca underfitting?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AdaBoost no tiene porqué ser árboles. \n",
    "Hay varios parámetros que podemos modificar: \n",
    "- n_estimators: número de estimadores (en AdaBoost no tiene porqué ser árboles) que participarán en la corrección secuencial del error del modelo\n",
    "- learning_rate:(aumentarlo) depende del número de n_estimators, por defecto es 1\n",
    "- max_depth: máxima profundidad del árbol\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pregunta 5\n",
    "**¿Qué podemos hacer si tenemos overfitting en GradientBoosting?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos modificar los mismos parámetros que para AdaBoost (pregunta anterior), pero aquí habrá más, ya que siempre serán árboles: \n",
    "- n_estimators\n",
    "- learning_rate (bajándolo)\n",
    "- max_depth\n",
    "- min_samples_split\n",
    "- min_samples_leaf\n",
    "- min_weight_fraction_leaf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
