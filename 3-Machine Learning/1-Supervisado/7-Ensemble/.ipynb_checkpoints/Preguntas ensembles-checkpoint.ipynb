{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicios de razonar\n",
    "Para estos ejercicios no hace falta escribir ni una línea de código. Simmplemente hay que responder a las preguntas.\n",
    "\n",
    "### Pregunta 1\n",
    "**Si entrenas 5 modelos diferentes y obtienes en los 5 un 95% de acierto, ¿sería posible combinar los 5 modelos para mejorar ese porcentaje?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entiendo que si es posible combinar esos 5 modelos con ensemble y que el modelo mejorará un poco."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pregunta 2\n",
    "**¿Cuál es la diferencia entre un hard y un soft voting classifier?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hard voting coge la predicción con el mayor número de votos, mientras que soft voting implica combinar las probabilidades de cada predicción en cada modelo y elegir la predicción con la probabilidad total más alta."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pregunta 3\n",
    "**Para mejorar los tiempos de entrenamiento, se suelen paralelizar los procesos de ejecución en el ordenador (un proceso por core) para que entrene más rápido. ¿Sería posible paralelizar el entrenamiento de los algoritmos de bagging vistos en clase? ¿Y los de boosting?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No lo creo posible, ya que bagging genera datos adicionales a partir de un conjunto de datos mediante la combinación de repeticiones con combinaciones de los datos originales. \n",
    "Boosting es una estrategia iterativa para ajustar el peso de una observación donde los datos van de manera secuencial. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pregunta 4\n",
    "**¿Qué podríamos modificar en nuestro algoritmo de AdaBoost para mejorarlo en el caso de que se produzca underfitting?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hay varios parámetros que podemos modificar: \n",
    "- n_estimators: número de árboles que participarán en la corrección secuencial del error del modelo\n",
    "- learning_rate: depende del número de n_estimators, por defecto es 1\n",
    "- max_depth: máxima profundidad del árbol\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pregunta 5\n",
    "**¿Qué podemos hacer si tenemos overfitting en GradientBoosting?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos modificar los 3 mismos parámetros que para AdaBoost (pregunta anterior): \n",
    "- n_estimators\n",
    "- learning_rate\n",
    "- max_depth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
